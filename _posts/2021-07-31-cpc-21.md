---
layout: post
title: "The 2021 Competitive Programming Contest (CPC ‚Äò21)"
date: 2021-07-31 16:45:10 -0400
Categories: [Competitive Programming]
tags: [Contest]
toc: true
comments: false
math: true
mermaid: true
---

Last week, the 2021 Competitive Programming Contest (CPC ‚Äò21) was held on [DMOJ](https://dmoj.ca/contest/cpc21c1). The editorials are published on DMOJ, so I won‚Äôt talk about solutions here. I‚Äôm here to talk about what goes into preparing a DMOJ contest and the mistakes we‚Äôve made with CPC ‚Äò21.

## Preparing a (DMOJ) Contest

### Creating Problems

Once you get your problem setters together, you‚Äôll need to create problems. Creating problems is a creative activity, which means that it‚Äôs hard to force yourself to create them on the spot. You‚Äôll need to use your diffused thinking to get ideas and only then realize those ideas into problems. I get my best problem ideas at night when I shower before going to bed üöøüõèÔ∏è. Since I would be too sleepy to think, I jot down my ideas and work on them the following morning üìù.

Your problems need to bring something new to the table. Good problems teach contestants new things they wouldn‚Äôt have otherwise learned. Most problem ideas are not interesting enough to put into a contest ‚ùå. Some contest platforms have higher quality requirements for contest problems. One example of a platform with a high standard is [Codeforces](https://codeforces.com/). For the [Codeforces LATOKEN Round 1](https://codeforces.com/blog/entry/91617), my team had around 20 rejected problems while 8 made their way into the contest. Some authors kept track of their problem rejection counts: [29/36 rejected](https://codeforces.com/blog/entry/87884) and [72/78 rejected](https://codeforces.com/blog/entry/79974). DMOJ has a lower standard than Codeforces, so we had an easier time creating our problems. AQT and I already had the problem ideas for P6 and P7 when we began work on CPC. As for P5, Plasmatic created many problems until we accepted the current one.

## Mistakes

### Problem 2

The data for this problem was very weak. Many incorrect solutions passed. Some of them even took quadratic time or worse. Given that this problem can have countless correct solutions, we could not be aware of all of them. There were also a ton of incorrect solutions that needed to be stopped. The data-creation process of this problem consisted of creating incorrect solutions and then generating data on which these incorrect solutions failed. Unfortunately, this was not enough to stop cheeses üßÄ from passing during the contest.

### Problem 5

Plasmatic forgot to turn off the interactor‚Äôs feedback when the contest began. All the feedback did was inform the contestant of how many queries his solution used on each test case, which wasn‚Äôt a big deal.

### Problem 6

I created this problem eight months ago, so I do not remember every detail about the test data. I created worst-case data to kill solutions that ran in $\mathcal{O}(N^2)$ or worse trying to take advantage of monotonicity. I also tightened up the time limit to kill solutions that ran in $\mathcal{O}(N \log^2 N)$. To kill greedy strategies, I wrote generators that forced answers where each of $A$, $B$, and $C$ needed to be non-zero.

On the morning of Day 1 of the contest, I found out that I forgot to enable short-circuiting. Short-circuiting stops judging subsequence subtasks as soon as one of them is wrong. `dalt` had a small bug in his code that made it fail subtask 1 but pass subtask 2. He ended up getting `60/100` in-contest and fixing his solution to get the full `100/100` after his window had ended. Since the contest had already started, I could not have turned on short-circuiting, as that would either be unfair to either later contestants or to `dalt`, depending on whether I let `dalt` keep his points. For now, I was not worried about the quality of the data.

In the afternoon, `Joelfu` submitted a solution that printed the minimum of the maximum values of $A$, $B$, and $C$. It failed the first subtask but passed the second one. I was shocked when I saw this. This was not simply due to my mistake of forgetting to turn on short-circuiting, but I had messed up the data! This had somehow slipped past my testing. I was nowworried that there would be more incoming cheeses üßÄ. Thankfully, there were none.

My mistake would have been caught if I had run invocations like I had done when creating problems on [Polygon](https://polygon.codeforces.com/). An invocation runs solutions on the test data and checks to ensure that the verdicts match what your solutions have been manually labeled as.

| Creating an invocation | Results of an invocation |
| - | - |
| ![a](/assets/img/content/cpc-21/invocation1.png) | ![a](/assets/img/content/cpc-21/invocation2.png) |

I don‚Äôt like using `testlib.h` and Polygon, so I have been relying on my own scripts. In the future, I think I‚Äôll write a script to automate validator checks.

Relevant meme by Plasmatic:

![meme](/assets/img/content/cpc-21/meme.png){:width="40%" }

## Closing Remarks

It was fun working on CPC ‚Äò21 and I‚Äôll continue to make contests in the future. I hope you enjoyed seeing the ‚Äúbehind the scenes‚Äù of creating a contest. Perhaps you've been inspired to create your own contest! Bye for now üëã!

 
