---
layout: post
title: "The 2021 Competitive Programming Contest (CPC ‚Äò21)"
date: 2021-07-31 16:45:10 -0400
Categories: [Competitive Programming]
tags: [Contest]
toc: true
comments: false
math: true
mermaid: true
---

Last week, the [2021 Competitive Programming Contest](https://dmoj.ca/contest/cpc21c1) (CPC ‚Äò21) was held on DMOJ. Editorials have been published, so I won‚Äôt talk about the solutions here. I‚Äôm here to talk about what goes into preparing a DMOJ contest and the mistakes we‚Äôve made with CPC ‚Äò21.

## Preparing a (DMOJ) Contest

### Creating Problems

Once you get your problem setters together, you‚Äôll need to make problems. Making problems is a creative activity, which meansthat it‚Äôs hard to force yourself to create them on the spot. I need to first use diffused thinking to get my ideas before switching to focused thinking to turn my ideas into problems. I get my best ideas at night when I shower before going to bed üöøüõèÔ∏è. I like to jot down my ideas to look over the following morning üìù.

Contest problems need to bring something new to the table. Good problems teach contestants things they wouldn‚Äôt have otherwise known. Most problem ideas are not interesting enough to put into a contest ‚ùå. One example of a platform with a high standard for problems is [Codeforces](https://codeforces.com/). For the [Codeforces LATOKEN Round 1](https://codeforces.com/blog/entry/91617), my team had around 20 rejected problems for the 8 that made it into the contest. Some authors had dozens of problems rejected for a single round: [29/36 rejected](https://codeforces.com/blog/entry/87884) and [72/78 rejected](https://codeforces.com/blog/entry/79974). DMOJ has a lower standard than Codeforces, so we had an easier time making our problems. AQT and I already had the ideas for P6 and P7 when we began work on CPC. As for P5, Plasmatic created many problems until we accepted the current one.

## Mistakes

### Problem 2

The data for this problem was very weak. Many incorrect solutions passed. Some of them even took quadratic time or worse. Given that this problem has countless correct solutions, we could not be aware of all of them. There were also a ton of incorrect solutions that needed to be stopped. The data-creation process for P2 consisted of creating incorrect solutions and then generating data on which they failed. Unfortunately, this was not enough to stop all the cheeses üßÄ from passing.

### Problem 5

Plasmatic forgot to turn off the interactor‚Äôs feedback when the contest began. All the feedback did was inform the contestant of how many queries his solution used on each test case, which wasn‚Äôt a big deal.

### Problem 6

I created this problem eight months ago, so I do not remember every detail about the test data. I created worst-case data to kill solutions that ran in $\mathcal{O}(N^2)$ or worse trying to take advantage of monotonicity. I also tightened up the time limit to kill solutions that ran in $\mathcal{O}(N \log^2 N)$. To kill greedy strategies, I wrote generators that forced answers where each of $A$, $B$, and $C$ needed to be non-zero.

On the morning of Day 1 of the contest, I found out that I forgot to enable short-circuiting. Short-circuiting stops judging subsequence subtasks as soon as one of them is wrong. `dalt` had a small bug in his code that made it fail subtask 1 but pass subtask 2. He ended up getting `60/100` in-contest and fixing his solution to get the full `100/100` after his window had ended. Since the contest had already started, I could not have turned on short-circuiting, as that would either be unfair to either later contestants or to `dalt`, depending on whether I let `dalt` keep his points. For now, I was not worried about the quality of the data.

In the afternoon, `Joelfu` submitted a solution that printed the minimum of the maximum values of $A$, $B$, and $C$. It failed the first subtask but passed the second one. I was shocked when I saw this. This was not simply due to my mistake of forgetting to turn on short-circuiting, but I had messed up the data! This had somehow slipped past my testing. I was now worried that there would be more incoming cheeses üßÄ. Thankfully, there were none.

My mistake would have been caught if I had run invocations like I had done when creating problems on [Polygon](https://polygon.codeforces.com/). An invocation runs solutions on the test data and checks to ensure that the verdicts match what your solutions have been manually labeled as.

| Creating an invocation | Results of an invocation |
| - | - |
| ![a](/assets/img/content/cpc-21/invocation1.png) | ![a](/assets/img/content/cpc-21/invocation2.png) |

I don‚Äôt like using `testlib.h` and Polygon, so I have been relying on my own scripts. In the future, I think I‚Äôll write a script to automate validator checks.

Relevant meme by Plasmatic:

![meme](/assets/img/content/cpc-21/meme.png){:width="40%" }

## Closing Remarks

It was fun working on CPC ‚Äò21 and I‚Äôll continue to make contests in the future. I hope you enjoyed seeing the ‚Äúbehind the scenes‚Äù of creating a contest. Perhaps you've been inspired to create your own contest! Bye for now üëã!

 
